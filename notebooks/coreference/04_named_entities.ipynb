{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/max/Projects/Coreference\n"
     ]
    }
   ],
   "source": [
    "%cd '/Users/max/Projects/Coreference/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/max/Projects/Coreference/rucoref\n",
      "/Users/max/Projects/Coreference\n"
     ]
    }
   ],
   "source": [
    "%cd 'rucoref'\n",
    "from anaphoralib.corpora import rueval\n",
    "from anaphoralib.tagsets import multeast\n",
    "from anaphoralib.tagsets.utils import same_grammemmes\n",
    "from anaphoralib.experiments import mentionpair\n",
    "from anaphoralib.experiments import coref_utils\n",
    "from anaphoralib import utils\n",
    "from anaphoralib.experiments import utils as exp_utils\n",
    "%cd '..'\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "scorer_path = 'rucoref/external/reference-coreference-scorers/scorer.pl'\n",
    "ruthes_path = '/Users/max/Datasets/ruthes-lite2/'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_clf(min_samples_leaf=0.005):\n",
    "    return DecisionTreeClassifier(random_state=random_state, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def dump_chains_in_corpus(corpus, test_chains, gold_mentions, out_file_name='coref_chains.txt'):\n",
    "    out_file = codecs.open(out_file_name, 'w', encoding='utf-8')\n",
    "    for i_text, text in enumerate(test_chains):\n",
    "        # Dumping SYS\n",
    "        for chain_id in test_chains[i_text]:\n",
    "            for elem_id in test_chains[i_text][chain_id]:\n",
    "                elem = gold_mentions[i_text][elem_id]\n",
    "                out_file.write(u'{text_id}\\tSYS\\t{chain_id}\\t{elem_id}\\t{offset}\\t{lemma}\\t{wordform}\\n'.format(text_id=corpus.doc_ids[i_text],\n",
    "                                                                                                                chain_id=chain_id,\n",
    "                                                                                                                elem_id=elem_id,\n",
    "                                                                                                                offset=elem.offset,\n",
    "                                                                                                                lemma=u' '.join(elem.lemma),\n",
    "                                                                                                                wordform=u' '.join(elem.wordform)\n",
    "                                                                                                               ))\n",
    "            out_file.write('\\n')\n",
    "        \n",
    "        # Dumping GS\n",
    "        for chain_id in corpus.gs[i_text]['chains'].keys():\n",
    "            #gs_mentions, gs_group_ids = coref_utils.get_gs_groups(corpus)\n",
    "            cur_gs_chain = {key: [gs_group_ids[i_text].index(item) for item in val]\n",
    "                            for key, val in corpus.gs[i_text]['chains'].items()}\n",
    "            for elem_id in cur_gs_chain[chain_id]:\n",
    "                elem = gs_mentions[i_text][elem_id]\n",
    "                out_file.write(u'{text_id}\\tGS \\t{chain_id}\\t{elem_id}\\t{offset}\\t{lemma}\\t{wordform}\\n'.format(text_id=corpus.doc_ids[i_text],\n",
    "                                                                                                                chain_id=chain_id,\n",
    "                                                                                                                elem_id=elem_id,\n",
    "                                                                                                                offset=elem.offset,\n",
    "                                                                                                                lemma=u' '.join(elem.lemma),\n",
    "                                                                                                                wordform=u' '.join(elem.wordform)\n",
    "                                                                                                               ))\n",
    "            out_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading RuThes-Lite:\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ElementTree\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dict_from_xml(filename, single_item_tag, tags):\n",
    "    items = {}\n",
    "    xml = ElementTree.parse(filename)\n",
    "    for item in xml.findall(single_item_tag):\n",
    "        items[int(item.attrib['id'])] = {tag: item.find(tag).text or '' for tag in tags}\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "concepts = load_dict_from_xml(os.path.join(ruthes_path, 'concepts.xml'), 'concept', ('name', 'gloss', 'domain'))\n",
    "entries = load_dict_from_xml(os.path.join(ruthes_path, 'text_entry.xml'), 'entry', ('name', 'lemma', 'synt_type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xml = ElementTree.parse(os.path.join(ruthes_path, 'synonyms.xml'))\n",
    "synonyms = collections.defaultdict(set)\n",
    "concept_index = {}\n",
    "\n",
    "for elem in xml.findall('entry_rel'):\n",
    "    synonyms[int(elem.attrib['concept_id'])].add(int(elem.attrib['entry_id']))\n",
    "    concept_index[entries[int(elem.attrib['entry_id'])]['lemma'].lower()] = int(elem.attrib['concept_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "ruthes_aliases = collections.defaultdict(set)\n",
    "for key in synonyms.keys():\n",
    "    val = synonyms[key]\n",
    "    good_entries = [entries[entry]['lemma'].lower() for entry in val if entry in entries and entries[entry]['synt_type'].startswith('N')]\n",
    "    for entry in good_entries:\n",
    "        ruthes_aliases[entry].update(good_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'городок' in ruthes_aliases[u'город']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relations = {}\n",
    "xml = ElementTree.parse(os.path.join(ruthes_path, 'relations.xml'))\n",
    "for item in xml.findall('rel'):\n",
    "    if item.attrib['name'] == u'ВЫШЕ':\n",
    "        rel_to = int(item.attrib['to'])\n",
    "        rel_from = int(item.attrib['from'])\n",
    "        #relations[concepts[rel_from]['name'].lower()] = concepts[rel_to]['name'].lower()\n",
    "        relations[rel_from] = rel_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_link(word1, word2):\n",
    "    link = False\n",
    "    \n",
    "    if word1 in concept_index and word2 in concept_index:\n",
    "        for word_from, word_to in ((word1, word2), (word2, word1)):\n",
    "            concept = concept_index[word_from]\n",
    "            target_concept = concept_index[word_to]\n",
    "            #print concept, concepts[concept]['name'], target_concept, concepts[target_concept]['name']\n",
    "            while concept in relations:\n",
    "                #print concept, target_concept\n",
    "                linked_concept = relations[concept]\n",
    "                if target_concept == concept:\n",
    "                    link = True\n",
    "                    break\n",
    "                concept = linked_concept\n",
    "            if link:\n",
    "                break\n",
    "            \n",
    "    \n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_link(u'вода', u'напиток')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing:\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rucoref_train = rueval.RuCorefCorpus(multeast, rueval)\n",
    "rucoref_test = rueval.RuCorefCorpus(multeast, rueval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 126\n",
      "Number of GS texts: 126\n",
      "Number of chains in a corpus: 2515\n",
      "Number of words in all chains: 11453\n"
     ]
    }
   ],
   "source": [
    "exp_utils.load_corpus(rucoref_train, 'Corpus-2015/Tokens.train.fixmorph.txt.parsed', 'Corpus-2015/Groups.train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 55\n",
      "Number of GS texts: 55\n",
      "Number of chains in a corpus: 1123\n",
      "Number of words in all chains: 5104\n"
     ]
    }
   ],
   "source": [
    "exp_utils.load_corpus(rucoref_test, 'Corpus-2015/Tokens.test.fixmorph.txt.parsed', 'Corpus-2015/Groups.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_ok = lambda g: g.tag.startswith('N') or (g.tag.startswith('P') and g.lemma[0] in multeast.coref_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_mentions, gs_group_ids = coref_utils.get_gs_groups(rucoref_test)\n",
    "gs_groups = gs_mentions\n",
    "\n",
    "pred_mentions, pred_group_ids = coref_utils.get_pred_groups(rucoref_test, group_ok)\n",
    "pred_groups = rucoref_test.groups\n",
    "\n",
    "pred_mentions_gold_bound, pred_gold_bounds_ids = coref_utils.get_pred_groups_gold_boundaries(rucoref_test, group_ok)\n",
    "pred_groups_gold_bound = rucoref_test.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_mentions_train, gs_group_ids_train = coref_utils.get_gs_groups(rucoref_train)\n",
    "gs_groups_train = gs_mentions_train\n",
    "\n",
    "pred_mentions_train, pred_group_ids_train = coref_utils.get_pred_groups(rucoref_train, group_ok)\n",
    "pred_groups_train = rucoref_train.groups\n",
    "\n",
    "pred_mentions_gold_bound_train, pred_gold_bounds_ids = coref_utils.get_pred_groups_gold_boundaries(rucoref_train, group_ok)\n",
    "pred_groups_gold_bound_train = rucoref_train.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the classifier:\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLMentionPairClassifier(mentionpair.MentionPairClassifier):\n",
    "    NEEDS_TRAINING = True\n",
    "    def __init__(self, scorer_path=None):\n",
    "        self.scorer_path = scorer_path\n",
    "    \n",
    "    def train(self, clf, corpus, mentions):\n",
    "        self.data_x = []\n",
    "        self.data_y = []\n",
    "        self.appositives = []\n",
    "        \n",
    "        self.tagset = corpus.tagset\n",
    "        \n",
    "        for i, text in enumerate(corpus.texts):\n",
    "            all_mentions = utils.find_mentions(corpus.groups[i], corpus.tagset)\n",
    "            gs = corpus.gs[i]\n",
    "            words_index = corpus.words_index[i]\n",
    "\n",
    "            \n",
    "            for chain_id in gs['chains']:\n",
    "                chain = gs['chains'][chain_id]\n",
    "                for pair in ((chain[i], chain[i+1]) for i in range(len(chain)-1)):\n",
    "                    text_groups = []\n",
    "                    for pair_elem in pair:\n",
    "                        gs_group = gs['groups'][pair_elem]\n",
    "                        \n",
    "                        words = [text[words_index[shift]] for shift in gs_group['tokens_shifts']]\n",
    "                        head = text[words_index[gs_group['head_shift'][0]]]\n",
    "                        text_groups.append(coref_utils.create_gs_group(gs_group, words, head))\n",
    "                    \n",
    "                    self.data_x.append(self.get_feature_vector(corpus.texts[i], corpus.parses[i] if corpus.parses else None, *text_groups))\n",
    "                    self.data_y.append(True)\n",
    "                    \n",
    "                    neg_first = None\n",
    "                    neg_last = None\n",
    "\n",
    "                    for i_mention, mention in enumerate(all_mentions):\n",
    "                        if mention.offset == text_groups[0].offset:\n",
    "                            neg_first = i_mention\n",
    "                        if mention.offset == text_groups[1].offset:\n",
    "                            neg_last = i_mention\n",
    "                        if neg_first and neg_last:\n",
    "                            break\n",
    "                    \n",
    "                    if not neg_first or not neg_last:\n",
    "                        continue\n",
    "                        \n",
    "                    neg_text_groups = all_mentions[neg_first+1:neg_last]\n",
    "                    #for neg_pair in ((neg_text_groups[i], neg_text_groups[i+1]) for i in range(len(neg_text_groups)-1)):\n",
    "                    #    self.data_x.append(self.get_feature_vector(corpus.texts[i], *neg_pair))\n",
    "                    #    self.data_y.append(False)\n",
    "                    for neg_group in neg_text_groups:\n",
    "                        self.data_x.append(self.get_feature_vector(corpus.texts[i], corpus.parses[i] if corpus.parses else None, neg_group, text_groups[1]))\n",
    "                        self.data_y.append(False)\n",
    "        \n",
    "        self.clf = clf\n",
    "        self.clf.fit(self.data_x, self.data_y)\n",
    "    \n",
    "    def pair_coreferent(self, pair, groups, words, parse):\n",
    "        vctr = self.get_feature_vector(words, parse, *pair)\n",
    "        return self.clf.predict([vctr])[0]\n",
    "    \n",
    "    def get_feature_vector(self, words, parse, group_1, group_2):\n",
    "        # group_1 — possible antecedent\n",
    "        # group_2 — anaphor\n",
    "        \n",
    "        vctr = []\n",
    "        feat_names = []\n",
    "        \n",
    "        self.feat_names = feat_names\n",
    "        return vctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MLMentionPairMoreFeatures(MLMentionPairClassifier):\n",
    "    NEEDS_TRAINING = True\n",
    "    \n",
    "    def __init__(self, scorer_path, feat_zones=None):\n",
    "        create_pro_rx = lambda strings: re.compile(ur'(^|\\b){}\\b'.format(u'|'.join(strings)))\n",
    "        self.scorer_path = scorer_path\n",
    "        self.feat_zones = feat_zones if feat_zones else tuple()\n",
    "        self.modif = set()\n",
    "        self.relatives = []\n",
    "        self.deictic_pronouns = []\n",
    "        self.str_match = []\n",
    "        self.modif_pairs = []\n",
    "        self.abbrs = []\n",
    "        self.synt_roles = []\n",
    "        self.rx_lat = re.compile('[A-Za-z]')\n",
    "        self.rx_endings = re.compile(u'(?<=[А-ЯЁа-яё])(а|ы|ой|е)(?= |$)')\n",
    "        \n",
    "        self.rx_pro_deictic = create_pro_rx((u'я', u'ты', u'мы', u'вы'))\n",
    "        self.rx_pro_personal = create_pro_rx((u'мой', u'твой', u'наш', u'ваш'))\n",
    "        self.rx_pro_reflexive = create_pro_rx((u\"свое\", \n",
    "                                               u\"своё\", u\"своего\", u\"своей\", u\"своем\", u\"своём\", u\"своему\", u\"своею\", u\"свой\", u\"свои\", \n",
    "                                               u\"своим\", u\"своими\", u\"своих\", u\"свою\", u\"своя\", u\"себе\", u\"себя\", u\"собой\", u\"собою\"))\n",
    "        self.rx_pro_possessive = create_pro_rx((u\"его\", u\"ее\", u\"её\", u\"ей\", u\"ему\", u\"ею\", u\"им\", u\"ими\", u\"их\"))\n",
    "    \n",
    "    def get_feature_vector(self, words, parse, group_1, group_2):\n",
    "        # group_1 — possible antecedent\n",
    "        # group_2 — anaphor\n",
    "        \n",
    "        head_1 = group_1.words[group_1.head] if group_1.type != 'word' else group_1\n",
    "        head_2 = group_2.words[group_2.head] if group_2.type != 'word' else group_2\n",
    "        \n",
    "        is_appo = False\n",
    "        \n",
    "        is_pronoun = lambda w: len(w.lemma) == 1 and w.lemma[0] in self.tagset.coref_pronouns\n",
    "        is_deictic_pronoun = lambda w: is_pronoun and self.tagset.extract_feature('person', w) in ('1', '2')\n",
    "        \n",
    "        is_proper = lambda w: self.tagset.extract_feature('proper', w) == 'p'\n",
    "        \n",
    "        group_1_proper = is_proper(group_1)\n",
    "        group_2_proper = is_proper(group_2)\n",
    "        \n",
    "        is_pronoun_1 = is_pronoun(group_1)\n",
    "        is_pronoun_2 = is_pronoun(group_2)\n",
    "        \n",
    "        number_agrees = lambda p: same_grammemmes('number', p, self.tagset)\n",
    "        gender_agrees = lambda p: same_grammemmes('gender', p, self.tagset)\n",
    "        animacity_agrees = lambda p: same_grammemmes('animate', p, self.tagset)\n",
    "        \n",
    "        person_1 = self.tagset.extract_feature('person', group_1)\n",
    "        person_2 = self.tagset.extract_feature('person', group_2)\n",
    "        \n",
    "        pronoun_persons = {u'мой': '1', u'наш': '1', u'я': '1', u'мы': '1', u'твой': 2, u'ваш': '2', u'ты': '2', u'вы': '2'}\n",
    "        \n",
    "        is_demonstrative = lambda w: [tag.startswith('Pd') or w.lemma[i] == u'этот' for i, tag in enumerate(w.tags)]\n",
    "        demonstr_1 = is_demonstrative(group_1) if len(group_1.lemma) > 1 else [0]\n",
    "        demonstr_2 = is_demonstrative(group_2) if len(group_2.lemma) > 1 else [0]\n",
    "        \n",
    "        filtered_lemmas_1 = [lemma for (i, lemma) in enumerate(group_1.lemma) if not demonstr_1[i]]\n",
    "        filtered_lemmas_2 = [lemma for (i, lemma) in enumerate(group_2.lemma) if not demonstr_2[i]]\n",
    "        \n",
    "        filtered_lemma_1 = ' '.join(filtered_lemmas_1)\n",
    "        filtered_lemma_2 = ' '.join(filtered_lemmas_2)\n",
    "        \n",
    "        filtered_wforms_1 = [wf for (i, wf) in enumerate(group_1.wordform) if not demonstr_1[i]]\n",
    "        filtered_wforms_2 = [wf for (i, wf) in enumerate(group_2.wordform) if not demonstr_2[i]]\n",
    "        \n",
    "        filtered_wf_1 = ' '.join(filtered_wforms_1)\n",
    "        filtered_wf_2 = ' '.join(filtered_wforms_2)\n",
    "        \n",
    "        modifiers_1 = [group_1.lemma[i] for i in range(group_1.head) if group_1.tags[i][0] == 'N']\n",
    "        modifiers_2 = [group_2.lemma[i] for i in range(group_2.head) if group_2.tags[i][0] == 'N']\n",
    "        \n",
    "        if filtered_lemma_1 in pronoun_persons:\n",
    "            person_1 = pronoun_persons[filtered_lemma_1]\n",
    "        if filtered_lemma_2 in pronoun_persons:\n",
    "            person_2 = pronoun_persons[filtered_lemma_2]\n",
    "        \n",
    "        person_agr = person_1 == person_2 if person_1 in {'1','2'} or person_2 in {'1', '2'} else -1\n",
    "        \n",
    "        n_sentences = -1\n",
    "        \n",
    "        self.modif.update(modifiers_1)\n",
    "        self.modif.update(modifiers_2)\n",
    "        \n",
    "        j = i = 0\n",
    "        if not head_1 in words or not head_2 in words:\n",
    "            n_sentences = -1\n",
    "            n_nouns = -1\n",
    "            dist_words = -1\n",
    "            print 'no alignment found'\n",
    "        else:\n",
    "            i = words.index(head_1)\n",
    "            j = words.index(head_2)\n",
    "            \n",
    "            gr1_end = i + len(group_1.lemma) - group_1.head - 1\n",
    "            gr2_start = j - group_2.head\n",
    "            \n",
    "            between_groups = words[gr1_end+1:gr2_start]\n",
    "            n_sentences = sum(1 for gr in between_groups if gr.tag == 'SENT')\n",
    "            n_nouns = sum(1 for gr in between_groups if gr.tag.startswith('N'))\n",
    "            dist_words = j - i\n",
    "            \n",
    "            if gr2_start - gr1_end == 2 and words[gr1_end+1].tag.startswith(',') \\\n",
    "                and same_grammemmes('case', (group_1, group_2), self.tagset) \\\n",
    "                and same_grammemmes('number', (group_1, group_2), self.tagset) \\\n",
    "                and same_grammemmes('gender', (group_1, group_2), self.tagset) \\\n",
    "                and group_1.tag.startswith('N') and group_2.tag.startswith('N'):\n",
    "                is_appo = True\n",
    "                self.appositives.append((group_1, group_2, i, j))\n",
    "                \n",
    "            # Capital letter heuristic\n",
    "            #if i > 0:\n",
    "            #    group_1_proper |= any(w[0].isupper() for w in group_1.wordform) and words[i-1].tag != 'SENT'\n",
    "            #if j > 0:\n",
    "            #    group_2_proper |= any(w[0].isupper() for w in group_2.wordform) and words[j-1].tag != 'SENT'\n",
    "                \n",
    "        # Endings heuristic\n",
    "        #if group_1_proper and group_2_proper:\n",
    "        #    str_match = self.rx_endings.sub(u'', filtered_lemma_1) == self.rx_endings.sub(u'', filtered_lemma_2)\n",
    "        #else:\n",
    "        #    str_match = filtered_lemma_1 == filtered_lemma_2\n",
    "        # No endings heuristic:\n",
    "        str_match = filtered_lemma_1 == filtered_lemma_2\n",
    "        \n",
    "        vctr = []\n",
    "        self.feat_names = []\n",
    "        \n",
    "        if 'soon' in self.feat_zones:\n",
    "            if not 'dist' in self.feat_zones:\n",
    "                vctr.append(n_sentences == 1)\n",
    "                self.feat_names.append('dist==1')\n",
    "\n",
    "            vctr.append(not is_pronoun_1 and not is_pronoun_2 and str_match)\n",
    "            self.feat_names.append('str_match')\n",
    "\n",
    "            is_animate_1 = self.tagset.extract_feature('animate', group_1) in ('y', 'a')\n",
    "            is_animate_2 = self.tagset.extract_feature('animate', group_2) in ('y', 'a')\n",
    "            sem_class_agreement = (is_animate_1 and is_animate_2) or (not is_animate_1 and not is_animate_2)\n",
    "            # Semantic similarity heuristic (head match)\n",
    "            if not is_pronoun_1:\n",
    "                sem_class_agreement &= group_1.lemma[group_1.head] == group_2.lemma[group_2.head]\n",
    "\n",
    "            vctr.append(sem_class_agreement)\n",
    "            self.feat_names.append('sem_class_agreement')\n",
    "            \n",
    "            if not 'morpho' in self.feat_zones:\n",
    "                vctr.append(is_pronoun_1)\n",
    "                vctr.append(is_pronoun_2)\n",
    "                self.feat_names.extend(('i_pronoun', 'j_pronoun'))\n",
    "\n",
    "                vctr.append(is_pronoun_1 and is_pronoun_2)\n",
    "                self.feat_names.append('both_pronouns')\n",
    "\n",
    "            vctr.append(number_agrees((group_1, group_2)))\n",
    "            vctr.append(gender_agrees((group_1, group_2)))\n",
    "            self.feat_names.extend(('number-agr', 'gender-agr'))\n",
    "\n",
    "            vctr.append(group_1_proper and group_2_proper)\n",
    "            self.feat_names.append('both-proper')\n",
    "            vctr.append(any(demonstr_2[:group_2.head+1]))\n",
    "            self.feat_names.append('anaphor-is-demonstrative')\n",
    "\n",
    "            vctr.append(is_appo)\n",
    "            self.feat_names.append('appositive')\n",
    "        \n",
    "        if 'morpho' in self.feat_zones:\n",
    "            #vctr.append(is_pronoun_1)\n",
    "            #vctr.append(is_pronoun_2)\n",
    "            #self.feat_names.extend(('i_pronoun', 'j_pronoun'))\n",
    "\n",
    "            #vctr.append(is_pronoun_1 and is_pronoun_2)\n",
    "            #self.feat_names.append('both_pronouns')\n",
    "            \n",
    "            vctr.append(self.rx_pro_deictic.search(filtered_lemma_2) is not None if is_pronoun_2 else -1)\n",
    "            vctr.append(self.rx_pro_deictic.search(filtered_lemma_1) is not None if is_pronoun_1 else -1)\n",
    "            self.feat_names.extend(('deictic_pronouns2', 'deictic_pronouns1'))\n",
    "            \n",
    "            #if vctr[-1] == True and vctr[-2] == True and filtered_lemma_1 == filtered_lemma_2:\n",
    "            #    self.deictic_pronouns.append((filtered_lemma_1, filtered_lemma_2))\n",
    "            #vctr.append(vctr[-1] == True and vctr[-2] == True and filtered_lemma_1 == filtered_lemma_2)\n",
    "            #self.feat_names.append('same_deictic_pronouns')\n",
    "            \n",
    "            vctr.append(self.rx_pro_personal.search(filtered_lemma_2) is not None if is_pronoun_2 else -1)\n",
    "            vctr.append(self.rx_pro_personal.search(filtered_lemma_1) is not None if is_pronoun_1 else -1)\n",
    "            \n",
    "            self.feat_names.append('pers_poss_pronouns2')\n",
    "            self.feat_names.append('pers_poss_pronouns1')\n",
    "            \n",
    "            #vctr.append(person_agr)\n",
    "            #vctr.append(((vctr[-1] == True and vctr[-3] == True) or (vctr[-2] == True and vctr[-4] == True)) and person_agr)\n",
    "            #self.feat_names.append('person_agr')\n",
    "            \n",
    "            vctr.append((filtered_lemma_2 in (u'который',) \n",
    "                        and (words[j-1].tag[0] == ',' or words[j-2].tag[0] == ',') \n",
    "                        and dist_words < 4) if is_pronoun_1 else -1)\n",
    "            if is_pronoun_2 and filtered_lemma_2 in (u'который',):\n",
    "                self.relatives.append((filtered_lemma_1, filtered_lemma_2, words[j-1], dist_words))\n",
    "            vctr.append(filtered_lemma_1 in (u'который',) if is_pronoun_2 else -1)\n",
    "            self.feat_names.append('rel_pronouns1')\n",
    "            self.feat_names.append('rel_pronouns2')\n",
    "            \n",
    "            vctr.append(self.rx_pro_reflexive.search(filtered_lemma_2) is not None if is_pronoun_2 else -1)\n",
    "            vctr.append(self.rx_pro_reflexive.search(filtered_lemma_1) is not None if is_pronoun_1 else -1)\n",
    "            self.feat_names.append('refl_pronouns2')\n",
    "            self.feat_names.append('refl_pronouns1')\n",
    "            \n",
    "            vctr.append(self.rx_pro_possessive.search(filtered_lemma_2) is not None if is_pronoun_2 else -1)\n",
    "            vctr.append(self.rx_pro_possessive.search(filtered_lemma_1) is not None if is_pronoun_1 else -1)\n",
    "            self.feat_names.append('poss_pronouns2')\n",
    "            self.feat_names.append('poss_pronouns1')\n",
    "            \n",
    "            #vctr.append(not is_pronoun_1 and not is_pronoun_2 \n",
    "            #            and (filtered_lemma_1.startswith(filtered_lemma_2)\n",
    "            #            or filtered_lemma_2.startswith(filtered_lemma_1)))\n",
    "            #self.feat_names.append('substring')\n",
    "        \n",
    "        if 'dist' in self.feat_zones:\n",
    "            vctr.append(n_sentences == 1)\n",
    "            self.feat_names.append('dist==1')\n",
    "            vctr.append(n_sentences > 2)\n",
    "            self.feat_names.append('dist>2')\n",
    "            vctr.append(n_nouns > 3)\n",
    "            self.feat_names.append('nouns>3')\n",
    "            \n",
    "        if 'lexical' in self.feat_zones:\n",
    "            vctr.append(not is_pronoun_1 and not is_pronoun_2 and len(modifiers_1) and filtered_lemma_2 == modifiers_1[0])\n",
    "            #vctr.append(not is_pronoun_1 and not is_pronoun_2 and len(modifiers_1) and filtered_lemma_2 in modifiers_1)\n",
    "            self.feat_names.append('modif-fullNP')\n",
    "            if vctr[-1] == True:\n",
    "                self.modif_pairs.append((' '.join(modifiers_1), filtered_lemma_1, filtered_lemma_2))\n",
    "            \n",
    "            is_abbr = (len(filtered_lemmas_1) > 1 or len(filtered_lemmas_2) > 1) and \\\n",
    "                        (''.join([w[0] for w in filtered_lemmas_1]) == filtered_lemma_2 or \\\n",
    "                         ''.join([w[0] for w in filtered_lemmas_2]) == filtered_lemma_1)\n",
    "            vctr.append(not is_pronoun_1 and not is_pronoun_2 and is_abbr)\n",
    "            if is_abbr:\n",
    "                self.abbrs.append((filtered_lemma_1, filtered_lemma_2))\n",
    "            self.feat_names.append('is_abbr')\n",
    "        \n",
    "        if 'synt' in self.feat_zones:\n",
    "            if parse:\n",
    "                synt_roles = []\n",
    "                for group, word_ind in ((group_1, i), (group_2, j)):\n",
    "                    #word_offset = group.words[group.head].offset if len(group.wordform) > 1 else group.offset\n",
    "                    synt_roles.append(parse[word_ind][1])\n",
    "                \n",
    "                is_subj_1 = synt_roles[0] == u'предик'\n",
    "                is_subj_2 = synt_roles[1] == u'предик'\n",
    "                is_obj_1 = synt_roles[0] in (u'1-компл', u'2-компл', u'предл')\n",
    "                is_obj_2 = synt_roles[1] in (u'1-компл', u'2-компл', u'предл')\n",
    "                \n",
    "                vctr.append(is_subj_1 and is_obj_2 and n_sentences == 0)\n",
    "                self.feat_names.append('subj_and_obj')\n",
    "                \n",
    "                #self.synt_roles.append((group_1, synt_roles[0], group_2, synt_roles[1]))\n",
    "            else:\n",
    "                is_subj_1 = self.tagset.extract_feature('case', group_1) == 'n' and words[i-1].tag == 'SENT'\n",
    "                is_subj_2 = self.tagset.extract_feature('case', group_2) == 'n' and words[j-1].tag == 'SENT'\n",
    "            \n",
    "            #vctr.append(is_subj_1)\n",
    "            #vctr.append(is_subj_2)\n",
    "            #self.feat_names.append('subj1')\n",
    "            #self.feat_names.append('subj2')\n",
    "            \n",
    "            vctr.append(words[i-1].tag == 'SENT')\n",
    "            vctr.append(words[j-1].tag == 'SENT')\n",
    "            self.feat_names.append('sent_start_1')\n",
    "            self.feat_names.append('sent_start_2')\n",
    "            \n",
    "            vctr.append(is_subj_1 == is_subj_2 and not is_pronoun_1 and is_pronoun_2 \n",
    "                        and self.tagset.extract_feature('person', group_2) == '3')\n",
    "            self.feat_names.append('subj_parallel')\n",
    "            #self.feat_names.append('both_subj')\n",
    "            #vctr.append(is_appo)\n",
    "            #self.feat_names.append('appo')\n",
    "        \n",
    "        return vctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NE_MLMentionPairClassifier(MLMentionPairMoreFeatures):\n",
    "    exceptions = {u'то', u'это', u'ваш', u'сам'}\n",
    "    def __init__(self, scorer_path, feat_zones=None, ne_list='ne.txt'):\n",
    "        self.aliases = []\n",
    "        self.domains = []\n",
    "        self.ne = []\n",
    "        self.w2v = []\n",
    "        super(NE_MLMentionPairClassifier, self).__init__(scorer_path=scorer_path, feat_zones=feat_zones)\n",
    "        self.read_ne(ne_list)\n",
    "        \n",
    "    def read_ne(self, filename):\n",
    "        self.entities = {}\n",
    "        self.entity_synonyms = {}\n",
    "        self.entity_types = {}\n",
    "        with codecs.open(filename, encoding='utf-8') as inp_file:\n",
    "            for i_entity, line in enumerate(inp_file):\n",
    "                if not line or line.startswith(';'):\n",
    "                    continue\n",
    "                ne_type, synonyms = line.strip('\\r\\n').split('|', 1)\n",
    "                synonyms = synonyms.split('|')\n",
    "                ne_type = ne_type.split(' ')[0]\n",
    "                \n",
    "                self.entities[i_entity] = (synonyms[0], ne_type, synonyms)\n",
    "                \n",
    "                for synonym in synonyms:\n",
    "                    if synonym in self.exceptions:\n",
    "                        continue\n",
    "                    if synonym not in self.entity_synonyms:\n",
    "                        self.entity_synonyms[synonym] = set()\n",
    "                        self.entity_types[synonym] = set()\n",
    "                        \n",
    "                    self.entity_synonyms[synonym].add(i_entity)\n",
    "                    self.entity_types[synonym].add(ne_type)\n",
    "    \n",
    "    def get_feature_vector(self, words, parse, group_1, group_2):\n",
    "        vctr = super(NE_MLMentionPairClassifier, self).get_feature_vector(words, parse, group_1, group_2)\n",
    "        lemma_1 = ' '.join(group_1.lemma)\n",
    "        lemma_2 = ' '.join(group_2.lemma)\n",
    "        \n",
    "        head_1 = group_1.lemma[group_1.head]\n",
    "        head_2 = group_2.lemma[group_2.head]\n",
    "        \n",
    "        sem_class_agreement = vctr[self.feat_names.index('sem_class_agreement')]\n",
    "        is_alias = -1\n",
    "        \n",
    "        is_pronoun_1 = lemma_1 in self.tagset.coref_pronouns\n",
    "        is_pronoun_2 = lemma_2 in self.tagset.coref_pronouns\n",
    "        \n",
    "        if 'sem' in self.feat_zones:\n",
    "            vctr[self.feat_names.index('sem_class_agreement')] = sem_class_agreement\n",
    "            if lemma_1 in self.entity_synonyms and lemma_2 in self.entity_synonyms and not is_pronoun_1 and not is_pronoun_2:\n",
    "                ent_1 = self.entity_synonyms[lemma_1]\n",
    "                ent_2 = self.entity_synonyms[lemma_2]\n",
    "\n",
    "                ent_types_1 = self.entity_types[lemma_1]\n",
    "                ent_types_2 = self.entity_types[lemma_2]\n",
    "\n",
    "                sem_class_agreement &= len(ent_types_1 & ent_types_2) > 0\n",
    "                is_alias = (len(ent_1 & ent_2) > 0, 'NE')\n",
    "\n",
    "                if lemma_1 != lemma_2:\n",
    "                    self.ne.append((lemma_1, lemma_2, is_alias[0]))\n",
    "            \n",
    "        if 'w2v' in self.feat_zones:\n",
    "            alias_w2v = -1\n",
    "            lemma_1_pos = head_1 + '_NOUN'\n",
    "            lemma_2_pos = head_2 + '_NOUN'\n",
    "            if head_1 != head_2 and not is_pronoun_1 and not is_pronoun_2 and lemma_1_pos in model and lemma_2_pos in model:\n",
    "                sim = model.similarity(lemma_1_pos, lemma_2_pos)\n",
    "                alias_w2v = sim > 0.85\n",
    "            \n",
    "            if alias_w2v > 0:\n",
    "                if is_alias == -1:\n",
    "                    is_alias = (alias_w2v, 'w2v')\n",
    "                else:\n",
    "                    is_alias = (is_alias[0] | alias_w2v, is_alias[1] + '|w2v')\n",
    "\n",
    "                self.w2v.append((lemma_1, lemma_2, sim))\n",
    "        \n",
    "        if 'ruthes' in self.feat_zones:\n",
    "            # checking ontological semantic class\n",
    "            if head_1 in concept_index and head_2 in concept_index:\n",
    "                concept_1 = concepts[concept_index[head_1]]\n",
    "                concept_2 = concepts[concept_index[head_2]]\n",
    "                if 'domain' in concept_1 and 'domain' in concept_2:\n",
    "                    vctr[self.feat_names.index('sem_class_agreement')] = concept_1['domain'] == concept_2['domain']\n",
    "                    self.domains.append((concept_1['domain'], head_1, head_2))\n",
    "            \n",
    "            # checking if NPs are aliases\n",
    "            is_ruthes_alias = -1\n",
    "            if head_1 in ruthes_aliases and head_1 != head_2:\n",
    "                is_ruthes_alias = head_2 in ruthes_aliases[head_1]\n",
    "            if head_2 in ruthes_aliases and head_1 != head_2:\n",
    "                is_ruthes_alias = head_1 in ruthes_aliases[head_2]\n",
    "            #else:\n",
    "            #    is_alias = find_link(head_1, head_2)\n",
    "            \n",
    "            if is_ruthes_alias > 0:\n",
    "                if is_alias == -1:\n",
    "                    is_alias = (is_ruthes_alias, 'ruthes')\n",
    "                else:\n",
    "                    is_alias = (is_alias[0] | is_ruthes_alias | find_link(head_1, head_2), is_alias[1] + '|ruthes')\n",
    "        \n",
    "        if is_alias == -1:\n",
    "            is_alias = (not is_pronoun_1 and not is_pronoun_2 and group_1.lemma[group_1.head] == group_2.lemma[group_2.head], 'default')\n",
    "        vctr.append(is_alias[0])\n",
    "        self.feat_names.append('alias')\n",
    "        \n",
    "        if is_alias[0]:\n",
    "            self.aliases.append((is_alias[1], lemma_1, lemma_2))\n",
    "        \n",
    "        return vctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline we will use a mention-pair classifier with all the features (see the previous NB for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{MLMentionPairMoreFeatures} &  $100.00$  & $79.29$ & $63.01$ & $70.22$  & $79.42$ & $48.39$ & $60.14$  & $53.65$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = MLMentionPairMoreFeatures(scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt'))\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An updated classifier without the new features shows slightly better results because of the `is_alias` default heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.35$ & $63.44$ & $70.51$  & $79.37$ & $48.60$ & $60.29$  & $53.83$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt'), ne_list='dictionaries/ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding NE lists to the classifier, replacing the head match as a semantic similarity heuristic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the small list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.39$ & $63.56$ & $70.60$  & $79.37$ & $48.72$ & $60.38$  & $53.93$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem'), ne_list='dictionaries/ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "рф россия\n",
      "евросоюз ес\n",
      "европа ес\n",
      "владимир путин путин\n",
      "евросоюз ес\n",
      "европа евросоюз\n",
      "ес евросоюз\n",
      "евросоюз европа\n",
      "ес европа\n",
      "ес европа\n",
      "евросоюз европа\n",
      "ес европа\n",
      "ес европа\n",
      "ес европа\n",
      "ес европа\n",
      "ес европа\n",
      "россия рф\n",
      "россия рф\n",
      "рф россия\n",
      "россия рф\n",
      "владимир путин путин\n",
      "рф россия\n",
      "международный телекоммуникационный союз international telecommunications union\n",
      "international telecommunications union itu\n",
      "рф россия\n",
      "россия рф\n"
     ]
    }
   ],
   "source": [
    "for ne in clf.ne:\n",
    "    if ne[2]:\n",
    "        print ne[0], ne[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NE рф россия\n",
      "NE евросоюз ес\n",
      "NE европа ес\n",
      "NE владимир путин путин\n",
      "NE евросоюз ес\n",
      "NE европа евросоюз\n",
      "NE ес евросоюз\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE россия рф\n",
      "NE россия рф\n",
      "NE рф россия\n",
      "NE россия рф\n",
      "NE владимир путин путин\n",
      "NE рф россия\n",
      "NE международный телекоммуникационный союз international telecommunications union\n",
      "NE international telecommunications union itu\n",
      "NE рф россия\n",
      "NE россия рф\n"
     ]
    }
   ],
   "source": [
    "for alias in clf.aliases:\n",
    "    if alias[0] != 'default' and alias[1] != alias[2]:\n",
    "        print alias[0], alias[1], alias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the full list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.43$ & $63.72$ & $70.71$  & $79.37$ & $48.86$ & $60.48$  & $54.05$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores, test_groups, test_chains = clf.score(rucoref_test, gs_mentions, gs_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_chains_in_corpus(rucoref_test, test_chains, gs_mentions, out_file_name='coref_chains_ne_full.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the word2vec feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('news.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.43$ & $63.72$ & $70.71$  & $79.37$ & $48.86$ & $60.48$  & $54.05$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.29$ & $63.49$ & $70.52$  & $79.25$ & $48.64$ & $60.28$  & $53.85$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'w2v'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "жена вера павловна свой двадцатисемилетнего муж 0.855655397458\n",
      "евросоюз ес 0.945408154903\n",
      "евросоюз ес 0.945408154903\n",
      "ес евросоюз 0.945408154903\n",
      "балтия прибалтика 0.883551623963\n",
      "самка самец 0.918114568386\n",
      "самка самец 0.918114568386\n",
      "самка самец 0.918114568386\n",
      "самка самец 0.918114568386\n",
      "самец самка 0.918114568386\n",
      "самец — глухой воркующее самка 0.918114568386\n",
      "седьмой студийный альбом английский рок - группа pink floyd пластинка 0.902718817777\n",
      "альбом пластинка 0.902718817777\n",
      "альбом пластинка 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "альбом новый пластинка 0.902718817777\n",
      "новый пластинка новый альбом 0.902718817777\n",
      "новый альбом данный пластинка 0.902718817777\n",
      "данный пластинка альбом 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "альбом пластинка 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "альбом пластинка 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "пластинка альбом 0.902718817777\n",
      "альбом этот пластинка 0.902718817777\n",
      "этот пластинка этот альбом 0.902718817777\n",
      "наталия александровна назимовой наталья васильевна 0.852146403813\n",
      "муж супруг 0.851049593152\n",
      "супруг муж 0.851049593152\n",
      "муж супруг 0.851049593152\n",
      "муж жена 0.855655397458\n",
      "жена муж 0.855655397458\n",
      "муж жена 0.855655397458\n",
      "она муж коварный супруг 0.851049593152\n",
      "коварный супруг неверный муж 0.851049593152\n",
      "видеоролик ролик 0.871916673568\n",
      "дебютный сольный альбом деймона албарна пластинка 0.902718817777\n",
      "его первый полноценный сольный альбом пластинка 0.902718817777\n"
     ]
    }
   ],
   "source": [
    "for ne in clf.w2v:\n",
    "    print ne[0], ne[1], ne[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.36$ & $63.77$ & $70.72$  & $79.25$ & $48.89$ & $60.47$  & $54.07$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem', 'w2v'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores, test_groups, test_chains = clf.score(rucoref_test, gs_mentions, gs_groups)\n",
    "dump_chains_in_corpus(rucoref_test, test_chains, gs_mentions, out_file_name='coref_chains_ne_w2v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v жена вера павловна свой двадцатисемилетнего муж\n",
      "NE россия российский федерация\n",
      "NE рф россия\n",
      "NE|w2v евросоюз ес\n",
      "NE европа ес\n",
      "NE сша америка\n",
      "NE сша америка\n",
      "NE владимир путин путин\n",
      "NE сша америка\n",
      "NE америка сша\n",
      "NE|w2v евросоюз ес\n",
      "NE европа евросоюз\n",
      "NE|w2v ес евросоюз\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE сша соединить штат\n",
      "NE ссср советский союз\n",
      "NE советский союз ссср\n",
      "NE петербург ленинград\n",
      "w2v балтия прибалтика\n",
      "NE россия российский федерация\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самец самка\n",
      "w2v самец — глухой воркующее самка\n",
      "NE россия рф\n",
      "NE советский союз ссср\n",
      "NE россия рф\n",
      "NE рф россия\n",
      "NE россия российский федерация\n",
      "NE сша америка\n",
      "w2v седьмой студийный альбом английский рок - группа pink floyd пластинка\n",
      "w2v альбом пластинка\n",
      "w2v альбом пластинка\n",
      "w2v пластинка альбом\n",
      "w2v альбом новый пластинка\n",
      "w2v новый пластинка новый альбом\n",
      "w2v новый альбом данный пластинка\n",
      "w2v данный пластинка альбом\n",
      "w2v пластинка альбом\n",
      "w2v пластинка альбом\n",
      "w2v альбом пластинка\n",
      "w2v пластинка альбом\n",
      "w2v альбом пластинка\n",
      "w2v пластинка альбом\n",
      "w2v пластинка альбом\n",
      "w2v пластинка альбом\n",
      "w2v альбом этот пластинка\n",
      "w2v этот пластинка этот альбом\n",
      "NE сша америка\n",
      "NE россия рф\n",
      "w2v наталия александровна назимовой наталья васильевна\n",
      "w2v муж супруг\n",
      "w2v супруг муж\n",
      "w2v муж супруг\n",
      "NE владимир путин путин\n",
      "NE сша америка\n",
      "w2v муж жена\n",
      "w2v жена муж\n",
      "w2v муж жена\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE сша америка\n",
      "NE америка сша\n",
      "NE соединить штат америка\n",
      "w2v она муж коварный супруг\n",
      "w2v коварный супруг неверный муж\n",
      "NE рф россия\n",
      "NE международный телекоммуникационный союз international telecommunications union\n",
      "NE international telecommunications union itu\n",
      "w2v видеоролик ролик\n",
      "NE рф россия\n",
      "NE россия рф\n",
      "w2v дебютный сольный альбом деймона албарна пластинка\n",
      "w2v его первый полноценный сольный альбом пластинка\n",
      "w2v наталия александровна назимовой наталья васильевна\n",
      "w2v муж супруг\n",
      "w2v супруг муж\n",
      "w2v муж супруг\n",
      "NE владимир путин путин\n",
      "NE сша америка\n",
      "w2v муж жена\n",
      "w2v жена муж\n",
      "w2v муж жена\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE сша америка\n",
      "NE америка сша\n",
      "NE соединить штат америка\n",
      "w2v она муж коварный супруг\n",
      "w2v коварный супруг неверный муж\n",
      "NE рф россия\n",
      "NE международный телекоммуникационный союз international telecommunications union\n",
      "NE international telecommunications union itu\n",
      "w2v видеоролик ролик\n",
      "NE рф россия\n",
      "NE россия рф\n",
      "w2v дебютный сольный альбом деймона албарна пластинка\n",
      "w2v его первый полноценный сольный альбом пластинка\n",
      "w2v наталия александровна назимовой наталья васильевна\n",
      "w2v муж супруг\n",
      "w2v супруг муж\n",
      "w2v муж супруг\n",
      "NE владимир путин путин\n",
      "NE сша америка\n",
      "w2v муж жена\n",
      "w2v жена муж\n",
      "w2v муж жена\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "NE сша америка\n",
      "NE америка сша\n",
      "NE соединить штат америка\n",
      "w2v она муж коварный супруг\n",
      "w2v коварный супруг неверный муж\n",
      "NE рф россия\n",
      "NE международный телекоммуникационный союз international telecommunications union\n",
      "NE international telecommunications union itu\n",
      "w2v видеоролик ролик\n",
      "NE рф россия\n",
      "NE россия рф\n",
      "w2v дебютный сольный альбом деймона албарна пластинка\n",
      "w2v его первый полноценный сольный альбом пластинка\n"
     ]
    }
   ],
   "source": [
    "for alias in clf.aliases:\n",
    "    if alias[0] != 'default' and alias[1] != alias[2]:\n",
    "        print alias[0], alias[1], alias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the impact of RuThes-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.19$ & $63.79$ & $70.66$  & $78.92$ & $48.78$ & $60.29$  & $53.87$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.14$ & $63.79$ & $70.64$  & $78.85$ & $48.78$ & $60.27$  & $53.87$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'w2v', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.24$ & $63.97$ & $70.79$  & $78.92$ & $48.94$ & $60.41$  & $54.01$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.36$ & $63.77$ & $70.72$  & $79.25$ & $48.89$ & $60.47$  & $54.07$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem', 'w2v'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $100.00$  & $79.19$ & $63.97$ & $70.77$  & $78.85$ & $48.94$ & $60.39$  & $54.01$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem', 'w2v', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, gs_mentions_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, gs_mentions, gs_groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruthes ситцевый платье платьице\n",
      "w2v жена вера павловна свой двадцатисемилетнего муж\n",
      "ruthes свой выдумка дурацкий сказка\n",
      "ruthes огонь пламя\n",
      "ruthes синий огонек пламя\n",
      "NE россия российский федерация\n",
      "ruthes рабочий место чиновник новостной служба\n",
      "ruthes дочка дочь\n",
      "ruthes государство страна\n",
      "NE|ruthes рф россия\n",
      "ruthes хамовническом районный суд год этот процесс\n",
      "ruthes суд этот процесс\n",
      "NE|w2v евросоюз ес\n",
      "NE европа ес\n",
      "NE|ruthes сша америка\n",
      "NE|ruthes сша америка\n",
      "NE владимир путин путин\n",
      "NE|ruthes сша америка\n",
      "NE|ruthes америка сша\n",
      "NE|w2v евросоюз ес\n",
      "NE европа евросоюз\n",
      "NE|w2v ес евросоюз\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE евросоюз европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "NE ес европа\n",
      "ruthes соглашение по изменение климат лиссабонский договор\n",
      "ruthes всеобъемлющий глобальный соглашение лиссабонский договор\n",
      "ruthes данный период тот пора\n",
      "NE сша соединить штат\n",
      "ruthes фото фотография\n",
      "ruthes фотография фото\n",
      "NE ссср советский союз\n",
      "NE советский союз ссср\n",
      "ruthes с. - петербург ленинград\n",
      "ruthes по возвращение в с. - петербург г.с ленинград\n",
      "ruthes санкт-петербург , @card@ ленинград\n",
      "NE|ruthes петербург ленинград\n",
      "ruthes малоизвестный широкий публика человек « немедийные лицо »\n",
      "ruthes человек немедийные лицо\n",
      "ruthes xxi столетие xxi век\n",
      "ruthes кино фильм\n",
      "w2v балтия прибалтика\n",
      "ruthes ваз автоваз\n",
      "ruthes автоваз ваз\n",
      "ruthes автоваз ваз\n",
      "ruthes весь страна государство\n",
      "NE россия российский федерация\n",
      "ruthes весьма слабый и тонкий колечко между они кольцо\n",
      "ruthes весьма слабый и тонкий колечко между они узкий кольцо уран\n",
      "ruthes ﻿небольшие колечко между широкий кольцо сатурн узкий кольцо уран\n",
      "ruthes весьма слабый и тонкий колечко между они ﻿эти слабый кольцо и пылевой полоса\n",
      "ruthes слабый и тонкий колечко ﻿эти слабый кольцо\n",
      "ruthes ﻿небольшие колечко между широкий кольцо сатурн пылевыми кольцо юпитер\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самка самец\n",
      "w2v самец самка\n",
      "w2v самец — глухой воркующее самка\n",
      "ruthes ﻿повседневное тв отечественный телевидение\n",
      "ruthes отечественный телевидение тв\n",
      "ruthes тв телевидение\n",
      "ruthes наш телевидение повседневный тв\n",
      "ruthes повседневный тв телевидение\n",
      "ruthes телевидение тв\n",
      "ruthes тв телевидение\n",
      "ruthes нервозные ощущение затаённое и несформулированное чувство вина\n",
      "ruthes ощущение затаённое и несформулированное чувство вина\n",
      "ruthes ощущение , что не по - таковски в наш ежедневности затаённое и несформулированное чувство вина\n",
      "ruthes государство страна\n",
      "ruthes брат и сестёр старик\n",
      "ruthes старик дед\n",
      "ruthes композиция подобный музыка\n",
      "ruthes работа свой труд\n",
      "ruthes государство страна\n",
      "NE|ruthes россия рф\n",
      "NE советский союз ссср\n",
      "NE|ruthes китай кнр\n",
      "NE|ruthes кнр китай\n",
      "ruthes телевидение тв\n",
      "ruthes наш тв телевидение\n",
      "ruthes телевидение нынешний тв\n",
      "ruthes тв телевидение\n",
      "NE|ruthes россия рф\n",
      "NE|ruthes рф россия\n",
      "NE россия российский федерация\n",
      "ruthes снимок фотография\n",
      "NE|ruthes сша америка\n",
      "w2v|ruthes седьмой студийный альбом английский рок - группа pink floyd пластинка\n",
      "w2v|ruthes альбом пластинка\n",
      "w2v|ruthes альбом пластинка\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes альбом новый пластинка\n",
      "w2v|ruthes новый пластинка новый альбом\n",
      "w2v|ruthes новый альбом данный пластинка\n",
      "w2v|ruthes данный пластинка альбом\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes альбом пластинка\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes альбом пластинка\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes пластинка альбом\n",
      "w2v|ruthes альбом этот пластинка\n",
      "w2v|ruthes этот пластинка этот альбом\n",
      "NE|ruthes сша америка\n",
      "NE|ruthes россия рф\n",
      "NE|ruthes кндр корея\n",
      "ruthes государство @card@ - миллионной страна\n",
      "ruthes @card@ - миллионной страна государство\n",
      "ruthes лицо человек с сигара\n",
      "ruthes его лицо человек с сигара\n",
      "ruthes пес кривоногой собака\n",
      "ruthes ребенок дитя\n",
      "ruthes мать мама\n",
      "ruthes дитя ребенок\n",
      "ruthes ребята неграмотный и не понимать алгебра ребенок\n",
      "ruthes герой публицистический радиопередача \" ясность \" наш героиня\n",
      "ruthes бабушка мой бабка\n",
      "ruthes дед старик\n",
      "w2v наталия александровна назимовой наталья васильевна\n",
      "w2v|ruthes муж супруг\n",
      "w2v|ruthes супруг муж\n",
      "w2v|ruthes муж супруг\n",
      "ruthes мой бабка мой новый бабушка марфа яковлевны разнатовской\n",
      "ruthes мой новый бабушка марфа яковлевны разнатовской добродушнейшей полный старушка\n",
      "ruthes специалист популярный эксперт\n",
      "NE владимир путин путин\n",
      "ruthes проблема с ядерный программа иран этот вопрос\n",
      "ruthes этот вопрос единственный проблема в пограничный страна европа\n",
      "NE|ruthes сша америка\n",
      "w2v муж жена\n",
      "w2v жена муж\n",
      "w2v муж жена\n",
      "ruthes компания фирма\n",
      "ruthes фирма компания\n",
      "ruthes весь страна государство\n",
      "ruthes государство страна\n",
      "ruthes страна государство\n",
      "ruthes государство страна\n",
      "NE соединить штат сша\n",
      "NE соединить штат сша\n",
      "ruthes авторитет мвф они влияние\n",
      "ruthes договор о нераспространение ядерный оружие соглашение\n",
      "ruthes страна государство - член европейский союз\n",
      "ruthes государство - член европейский союз этот два страна\n",
      "NE соединить штат сша\n",
      "NE|ruthes сша америка\n",
      "NE|ruthes америка сша\n",
      "ruthes гуру это место свой учитель\n",
      "NE соединить штат америка\n",
      "w2v|ruthes она муж коварный супруг\n",
      "w2v|ruthes коварный супруг неверный муж\n",
      "NE|ruthes рф россия\n",
      "ruthes этот должность пост главный тренер\n",
      "ruthes первый дивизион премьер - лига\n",
      "NE международный телекоммуникационный союз international telecommunications union\n",
      "NE international telecommunications union itu\n",
      "w2v|ruthes видеоролик ролик\n",
      "NE|ruthes рф россия\n",
      "NE|ruthes россия рф\n",
      "ruthes « человек толпа » лицо который\n",
      "ruthes лицо это бродяга личность\n",
      "w2v|ruthes дебютный сольный альбом деймона албарна пластинка\n",
      "w2v|ruthes его первый полноценный сольный альбом пластинка\n"
     ]
    }
   ],
   "source": [
    "for alias in clf.aliases:\n",
    "    if alias[0] != 'default' and alias[1] != alias[2]:\n",
    "        print alias[0], alias[1], alias[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping the groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores, test_groups, test_chains = clf.score(rucoref_test, gs_mentions, gs_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_chains_in_corpus(rucoref_test, test_chains, gs_mentions, out_file_name='coref_chains_all_sem.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The impact of various combinations and comparing semantic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$79.35$ & $63.44$ & $70.51$  & $79.37$ & $48.60$ & $60.29$  & $53.83$: Without semantics\n",
    "$79.43$ & $63.72$ & $70.71$  & $79.37$ & $48.86$ & $60.48$  & $54.05$: With NE\n",
    "$79.29$ & $63.49$ & $70.52$  & $79.25$ & $48.64$ & $60.28$  & $53.85$: With word2vec\n",
    "$79.19$ & $63.79$ & $70.66$  & $78.92$ & $48.78$ & $60.29$  & $53.87$: With RuThes\n",
    "$79.36$ & $63.77$ & $70.72$  & $79.25$ & $48.89$ & $60.47$  & $54.07$: With NE & word2vec\n",
    "$79.24$ & $63.97$ & $70.79$  & $78.92$ & $48.94$ & $60.41$  & $54.01$: With NE & RuThes\n",
    "$79.14$ & $63.79$ & $70.64$  & $78.85$ & $48.78$ & $60.27$  & $53.87$: With word2vec & RuThes\n",
    "$79.19$ & $63.97$ & $70.77$  & $78.85$ & $48.94$ & $60.39$  & $54.01$: All semantics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the values on predicted mentions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $38.05$ & $56.56$ & $45.49$  & $21.79$ & $44.33$ & $29.22$  & $27.09$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon',), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $37.94$ & $53.87$ & $44.52$  & $25.00$ & $42.61$ & $31.51$  & $27.47$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $38.01$ & $54.10$ & $44.65$  & $24.99$ & $42.83$ & $31.56$  & $27.57$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $37.69$ & $53.92$ & $44.37$  & $24.95$ & $42.68$ & $31.49$  & $27.51$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'w2v'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $36.27$ & $54.20$ & $43.46$  & $24.63$ & $42.83$ & $31.28$  & $27.45$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textsc{NE_MLMentionPairClassifier} &  $51.19$  & $36.08$ & $54.32$ & $43.36$  & $24.60$ & $42.94$ & $31.28$  & $27.51$ \\\\\n"
     ]
    }
   ],
   "source": [
    "clf = NE_MLMentionPairClassifier(scorer_path=scorer_path, feat_zones=('soon', 'morpho', 'dist', 'lexical', 'synt', 'sem', 'w2v', 'ruthes'), ne_list='dictionaries/all-ne.txt')\n",
    "clf.train(create_clf(), rucoref_train, pred_mentions_gold_bound_train)\n",
    "coref_utils.get_score_table(clf, rucoref_test, pred_mentions_gold_bound, pred_groups_gold_bound, False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
